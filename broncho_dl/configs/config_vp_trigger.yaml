# config_basisline_di_hysteresis_modelling.yaml


defaults:
- datasets: vp_trigger
- models: vp_training/vp_trigger
- optimizers: adam_cosine
- pl_modules: vp_training/vp_trigger
- trainers: vp_training
- _self_


# General settings
task_name: "vp_training_trigger_predictor"
variable_name: trainers.launch_trainer.repeat_trial
output_name: baseline_debug
seed: 42
results_dir: '/home/haoj/vp_trigger'
data_folder_path: '/home/haoj/0/'

hydra:
  sweeper:
    params:
      trainers.launch_trainer.repeat_trial: 1
      trainers.launch_trainer.max_epochs: 200
      trainers.launch_trainer.monitor:  "val_total_loss"
      datasets.dataloader.data_folder: ${data_folder_path}
      datasets.dataloader.batch_size: 32

  sweep:  # didn't use ${hydra.runtime.choices.pl_modules} because "/" \\ if add timestamp: _${now:%m-%d-%H:%M:%S}
    dir: ${results_dir}/${task_name}/${datasets.name}/${output_name}/${pl_modules.name}_${models.name}_${optimizers.name}/
    # only controls where config.yaml is saved
    subdir: ./
  run: # control where multirun.yaml is saved
    dir: ${hydra.sweep.sub}/.hydra







